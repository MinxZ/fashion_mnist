{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.applications import *\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import *\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tqdm import tqdm\n",
    "from keras import backend as K\n",
    "from skimage.transform import resize\n",
    "\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    num_classes = 10\n",
    "    height,width = 128, 128\n",
    "        \n",
    "    (train, train_l), (test, test_l) = fashion_mnist.load_data()\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y = keras.utils.to_categorical(train_l, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_l, num_classes)\n",
    "    \n",
    "    \n",
    "    # use this in future\n",
    "    # X_train = np.array([resize(x, (height,width)).astype(float) for x in tqdm(iter(X_train.astype(int)))])/255.\n",
    "    \n",
    "    train = train.reshape((-1,28,28))\n",
    "    train = np.array([misc.imresize(x, (height,width)) for x in tqdm(iter(train))])\n",
    "    test = test.reshape((-1,28,28))\n",
    "    test = np.array([misc.imresize(x, (height,width)) for x in tqdm(iter(test))])\n",
    "\n",
    "    x = np.stack((train, train, train), axis=3)\n",
    "    x_test = np.stack((test, test, test), axis=3)\n",
    "    \n",
    "    print(x.shape)\n",
    "    # devide into train and validation \n",
    "    dvi = int(train.shape[0] * 0.9)\n",
    "    x_train = x[:dvi, :, :, :]\n",
    "    y_train = y[:dvi, :]\n",
    "    x_val = x[dvi:, :, :, :]\n",
    "    y_val = y[dvi:, :]\n",
    "    \n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_val.shape[0], 'validation samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "60000it [00:06, 9804.27it/s]\n",
      "0it [00:00, ?it/s]/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "10000it [00:00, 10175.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 128, 128, 3)\n",
      "x_train shape: (54000, 128, 128, 3)\n",
      "54000 train samples\n",
      "6000 validation samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run(model_name, lr, optimizer, epoch, patience, batch_size):\n",
    "    # Loading Datasets\n",
    "    (x_train, y_train), (x_val, y_val), (x_test, y_test) = load_data()\n",
    "\n",
    "    # Compute the bottleneck feature\n",
    "    \n",
    "    n_class = y_test.shape[1]\n",
    "    input_shape = (height, width, 3)\n",
    "    weights=None\n",
    "\n",
    "    def get_features(MODEL, data=x_train):\n",
    "        cnn_model = MODEL(\n",
    "            include_top=False,\n",
    "            input_shape=input_shape,\n",
    "            weights=weights)\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "        x = inputs\n",
    "        x = Lambda(preprocess_input, name='preprocessing')(x)\n",
    "        x = cnn_model(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        cnn_model = Model(inputs, x)\n",
    "\n",
    "        features = cnn_model.predict(data, batch_size=32, verbose=1)\n",
    "        return features\n",
    "\n",
    "    def fine_tune(MODEL,\n",
    "                  model_name,\n",
    "                  optimizer,\n",
    "                  lr,\n",
    "                  epoch,\n",
    "                  patience,\n",
    "                  batch_size,\n",
    "                  X=x_train):\n",
    "        # Fine-tune the model\n",
    "        print(\"\\n\\n Fine tune \" + model_name + \" : \\n\")\n",
    "        \n",
    "        if weights != None:\n",
    "            try:\n",
    "                model.load_weights(model_name + '.h5')\n",
    "                print('Load ' + model_name + '.h5 successfully.')\n",
    "            except:\n",
    "                try:\n",
    "                    model.load_weights('fc_' + model_name + '.h5', by_name=True)\n",
    "                    print('Fail to load ' + model_name + '.h5, load fc_' +\n",
    "                          model_name + '.h5 instead.')\n",
    "                except:\n",
    "                    print(\n",
    "                        'Start computing ' + model_name + ' bottleneck feature: ')\n",
    "                    features = get_features(MODEL, X)\n",
    "\n",
    "                    # Training models\n",
    "                    inputs = Input(features.shape[1:])\n",
    "                    x = inputs\n",
    "                    x = Dropout(0.5)(x)\n",
    "                    x = Dense(n_class, activation='softmax', name='predictions')(x)\n",
    "                    model_fc = Model(inputs, x)\n",
    "                    model_fc.compile(\n",
    "                        optimizer='adam',\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "                    h = model_fc.fit(\n",
    "                        features,\n",
    "                        y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=5,\n",
    "                        validation_split=0.1)\n",
    "\n",
    "                    model_fc.save('fc_' + model_name + '.h5', 'w')\n",
    "                    \n",
    "        datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocess_input,\n",
    "            horizontal_flip=True)\n",
    "        val_datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocess_input)\n",
    "\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "        x = inputs\n",
    "        cnn_model = MODEL(\n",
    "            include_top=False, input_shape=input_shape, weights=None)\n",
    "        x = cnn_model(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(n_class, activation='softmax', name='predictions')(x)\n",
    "        model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "\n",
    "        # for layer in model.layers[:114]:\n",
    "        #     layer.trainable = False\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"\\n \" + \"Optimizer=\" + optimizer + \" lr=\" + str(lr) + \" \\n\")\n",
    "        \n",
    "        if optimizer == \"Adam\":\n",
    "            model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "        elif optimizer == \"SGD\":\n",
    "            model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer=SGD(lr=lr, momentum=0.9, nesterov=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "        class LossHistory(keras.callbacks.Callback):\n",
    "            def on_train_begin(self, logs={}):\n",
    "                # self.val_losses = []\n",
    "                self.losses = []\n",
    "\n",
    "            def on_epoch_end(self, batch, logs={}):\n",
    "                # self.val_losses.append(logs.get(\"val_loss\"))\n",
    "                self.losses.append((logs.get('loss'), logs.get(\"val_loss\")))\n",
    "        \n",
    "#         model.summary()\n",
    "        history = LossHistory()\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss', patience=patience, verbose=1, mode='auto')\n",
    "        checkpointer = ModelCheckpoint(\n",
    "            filepath=model_name + '.h5', verbose=0, save_best_only=True)\n",
    "        h2 = model.fit_generator(\n",
    "            datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "            steps_per_epoch=len(x_train) / batch_size,\n",
    "            validation_data=val_datagen.flow(\n",
    "                x_val, y_val, batch_size=batch_size),\n",
    "            validation_steps=len(x_val) / batch_size,\n",
    "            epochs=epoch,\n",
    "            callbacks=[early_stopping, checkpointer, history])\n",
    "        \n",
    "        with open(model_name + \".csv\", 'a') as f_handle:\n",
    "            np.savetxt(f_handle, history.losses)\n",
    "        \n",
    "    list_model = {\n",
    "        \"Xception\": Xception,\n",
    "        \"InceptionV3\": InceptionV3,\n",
    "        \"InceptionResNetV2\": InceptionResNetV2,\n",
    "        \"VGG16\": VGG16,\n",
    "        \"MobileNet\": MobileNet\n",
    "    }\n",
    "    print(list_model[model_name])\n",
    "    fine_tune(list_model[model_name], model_name, optimizer, lr, epoch,\n",
    "              patience, batch_size, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "60000it [00:05, 10036.79it/s]\n",
      "0it [00:00, ?it/s]/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "10000it [00:00, 10153.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 128, 128, 3)\n",
      "x_train shape: (54000, 128, 128, 3)\n",
      "54000 train samples\n",
      "6000 validation samples\n",
      "10000 test samples\n",
      "<function Xception at 0x7fe99187c5f0>\n",
      "\n",
      "\n",
      " Fine tune Xception : \n",
      "\n",
      "\n",
      " Optimizer=Adam lr=3e-05 \n",
      "\n",
      "Epoch 1/10000\n",
      "438/843 [==============>...............] - ETA: 4:00 - loss: 0.4790 - acc: 0.8264"
     ]
    }
   ],
   "source": [
    "run(\"MobileNet\", 1e-2, \"Adam\", 10000, 5, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training preprocess visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(h.history['loss'])\n",
    "plt.plot(h.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(h.history['acc'])\n",
    "plt.plot(h.history['val_acc'])\n",
    "plt.legend(['acc', 'val_acc'])\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
