{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.applications import *\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.callbacks import *\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import *\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from scipy import misc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    num_classes = 10\n",
    "    height,width = 56, 56\n",
    "        \n",
    "    (train, train_l), (test, test_l) = fashion_mnist.load_data()\n",
    "\n",
    "    y = keras.utils.to_categorical(train_l, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_l, num_classes)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     train = train.reshape((-1,28,28))\n",
    "#     print(train.shape)\n",
    "#     train = np.array([misc.imresize(x, (height,width)) for x in tqdm(iter(train))])\n",
    "#     test = test.reshape((-1,28,28))\n",
    "#     test = np.array([misc.imresize(x, (height,width)) for x in tqdm(iter(test))])\n",
    "\n",
    "#     x = np.stack((train, train, train), axis=3)\n",
    "#     x_test = np.stack((test, test, test), axis=3)\n",
    "    x = train.reshape((train.shape[0],28,28,1))\n",
    "    x_test = test.reshape((test.shape[0],28,28,1))\n",
    "\n",
    "    # devide into train and validation \n",
    "    dvi = int(train.shape[0] * 0.9)\n",
    "    x_train = x[:dvi, :, :, :]\n",
    "    y_train = y[:dvi, :]\n",
    "    x_val = x[dvi:, :, :, :]\n",
    "    y_val = y[dvi:, :]\n",
    "    \n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_val.shape[0], 'validation samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (54000, 28, 28, 1)\n",
      "54000 train samples\n",
      "6000 validation samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Loading Datasets\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model_name, lr, optimizer, epoch, patience, batch_size, weights, test):\n",
    "    n_class = y_test.shape[1]\n",
    "    input_shape = x_train.shape[1:]\n",
    "    if weights == 'None':\n",
    "        weights = None\n",
    "    else:\n",
    "        weights = 'imagenet'\n",
    "    print('weights are ' + str(weights))\n",
    "\n",
    "    def get_features(MODEL, data=x_train):\n",
    "        cnn_model = MODEL(\n",
    "            include_top=False, input_shape=input_shape, weights=weights)\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "        x = inputs\n",
    "        x = Lambda(preprocess_input, name='preprocessing')(x)\n",
    "        x = cnn_model(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        cnn_model = Model(inputs, x)\n",
    "\n",
    "        features = cnn_model.predict(data, batch_size=32, verbose=1)\n",
    "        return features\n",
    "\n",
    "    def fine_tune(MODEL,\n",
    "                  model_name,\n",
    "                  optimizer,\n",
    "                  lr,\n",
    "                  epoch,\n",
    "                  patience,\n",
    "                  batch_size,\n",
    "                  weights,\n",
    "                  X=x_train):\n",
    "        # Fine-tune the model\n",
    "        print(\"\\n Fine tune \" + model_name + \" : \\n\")\n",
    "\n",
    "        from random_eraser import get_random_eraser\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=True,\n",
    "            featurewise_std_normalization=True,\n",
    "            horizontal_flip=True,\n",
    "            preprocessing_function=get_random_eraser(v_h=60, pixel_level=False))\n",
    "#         ,\n",
    "#             width_shift_range=0.2,\n",
    "#             height_shift_range=0.2)\n",
    "\n",
    "        val_datagen = ImageDataGenerator(\n",
    "            featurewise_center=True, featurewise_std_normalization=True)\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "        x = inputs\n",
    "        cnn_model = MODEL(\n",
    "            include_top=False, input_shape=input_shape, weights=None)\n",
    "        x = cnn_model(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(n_class, activation='softmax', name='predictions')(x)\n",
    "        model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "        # Loading weights\n",
    "        try:\n",
    "            model.load_weights(model_name + '.h5')\n",
    "            print('Load ' + model_name + '.h5 successfully.')\n",
    "        except:\n",
    "            if weights == 'imagenet':\n",
    "                print('Start computing ' + model_name +\n",
    "                      ' bottleneck feature: ')\n",
    "                features = get_features(MODEL, X)\n",
    "\n",
    "                # Training models\n",
    "                inputs = Input(features.shape[1:])\n",
    "                x = inputs\n",
    "                x = Dropout(0.5)(x)\n",
    "                x = Dense(\n",
    "                    n_class, activation='softmax', name='predictions')(x)\n",
    "                model_fc = Model(inputs, x)\n",
    "                model_fc.compile(\n",
    "                    optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "                h = model_fc.fit(\n",
    "                    features,\n",
    "                    y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=5,\n",
    "                    validation_split=0.1)\n",
    "                model_fc.save('fc_' + model_name + '.h5')\n",
    "                model.load_weights(\n",
    "                    'fc_' + model_name + '.h5', by_name=True)\n",
    "\n",
    "\n",
    "        print(\"\\n \" + \"Optimizer=\" + optimizer + \" lr=\" + str(lr) + \" \\n\")\n",
    "        if optimizer == \"Adam\":\n",
    "            model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "        elif optimizer == \"SGD\":\n",
    "            model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer=SGD(lr=lr, momentum=0.9, nesterov=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "        if not test:\n",
    "            datagen.fit(x_train)\n",
    "            val_datagen.fit(x_val)\n",
    "            class LossHistory(keras.callbacks.Callback):\n",
    "                def on_train_begin(self, logs={}):\n",
    "                    # self.val_losses = []\n",
    "                    self.losses = []\n",
    "\n",
    "                def on_epoch_end(self, batch, logs={}):\n",
    "                    # self.val_losses.append(logs.get(\"val_loss\"))\n",
    "                    self.losses.append((logs.get('loss'), logs.get(\"val_loss\")))\n",
    "\n",
    "            history = LossHistory()\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss', patience=patience, verbose=1, mode='auto')\n",
    "            checkpointer = ModelCheckpoint(\n",
    "                filepath=model_name + '.h5', verbose=0, save_best_only=True)\n",
    "            reduce_lr = ReduceLROnPlateau(factor=0.333, patience=3,verbose=1)\n",
    "\n",
    "            h2 = model.fit_generator(\n",
    "                datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                steps_per_epoch=len(x_train) / batch_size,\n",
    "                validation_data=val_datagen.flow(\n",
    "                    x_val, y_val, batch_size=batch_size),\n",
    "                validation_steps=len(x_val) / batch_size,\n",
    "                epochs=epoch,\n",
    "                callbacks=[history, early_stopping, checkpointer, reduce_lr])\n",
    "\n",
    "            with open(model_name + \".csv\", 'a') as f_handle:\n",
    "                np.savetxt(f_handle, history.losses)\n",
    "        else:\n",
    "            print('Evalute on test set')\n",
    "            val_datagen.fit(x_test)\n",
    "            score = model.evaluate_generator(val_datagen.flow(x_test, y_test, batch_size=batch_size),\n",
    "                len(x_test) / batch_size)\n",
    "            print(score)\n",
    "            return score\n",
    "            \n",
    "\n",
    "    list_model = {\n",
    "        \"Xception\": Xception,\n",
    "        \"InceptionV3\": InceptionV3,\n",
    "        \"InceptionResNetV2\": InceptionResNetV2,\n",
    "        \"VGG16\": VGG16,\n",
    "        \"MobileNet\": MobileNet\n",
    "    }\n",
    "    fine_tune(list_model[model_name], model_name, optimizer, lr, epoch,\n",
    "              patience, batch_size, weights, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights are None\n",
      "\n",
      " Fine tune MobileNet : \n",
      "\n",
      "test\n",
      "Load MobileNet.h5 successfully.\n",
      "\n",
      " Optimizer=SGD lr=0.0005 \n",
      "\n",
      "Evalute on test set\n",
      "[0.21463979122638702, 0.9244]\n"
     ]
    }
   ],
   "source": [
    "run(\"MobileNet\", 5e-4, \"SGD\", 10000, 5, 128, 'None', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
