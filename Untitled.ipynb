{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.applications import *\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import *\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tqdm import tqdm\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data():\n",
    "    num_classes = 10\n",
    "    img_rows, img_cols = 28, 28\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    # train is data, train_l is label for train data\n",
    "    (train, train_l), (test, test_l) = fashion_mnist.load_data()\n",
    "    X = train.reshape((train.shape[0],) + input_shape)\n",
    "    x_test = test.reshape((test.shape[0],) + input_shape)\n",
    "    X = X.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y = keras.utils.to_categorical(train_l, num_classes)\n",
    "    y_test = keras.utils.to_categorical(test_l, num_classes)\n",
    "    \n",
    "    # devide into train and validation \n",
    "    dvi = int(X.shape[0] * 0.9)\n",
    "    x_train = X[:dvi, :, :, :]\n",
    "    y_train = y[:dvi, :]\n",
    "    x_val = X[dvi:, :, :, :]\n",
    "    y_val = y[dvi:, :]\n",
    "    \n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_val.shape[0], 'validation samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (54000, 28, 28, 1)\n",
      "54000 train samples\n",
      "6000 validation samples\n",
      "10000 test samples\n",
      "<function MobileNet at 0x7fca1f41b410>\n",
      "\n",
      "\n",
      " Fine tune MobileNet : \n",
      "\n",
      "Load MobileNet.h5 successfully.\n",
      "\n",
      " Optimizer=SGD lr=0.0005 \n",
      "\n",
      "Epoch 1/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8090Epoch 00001: val_loss improved from inf to 0.37937, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 21s 50ms/step - loss: 0.5348 - acc: 0.8091 - val_loss: 0.3794 - val_acc: 0.8563\n",
      "Epoch 2/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.8173Epoch 00002: val_loss improved from 0.37937 to 0.37205, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.5124 - acc: 0.8173 - val_loss: 0.3720 - val_acc: 0.8573\n",
      "Epoch 3/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.5076 - acc: 0.8211Epoch 00003: val_loss improved from 0.37205 to 0.36940, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.5079 - acc: 0.8210 - val_loss: 0.3694 - val_acc: 0.8593\n",
      "Epoch 4/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4984 - acc: 0.8234- ETA: Epoch 00004: val_loss improved from 0.36940 to 0.36725, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4986 - acc: 0.8234 - val_loss: 0.3672 - val_acc: 0.8592\n",
      "Epoch 5/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4961 - acc: 0.8234Epoch 00005: val_loss improved from 0.36725 to 0.36662, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4961 - acc: 0.8234 - val_loss: 0.3666 - val_acc: 0.8592\n",
      "Epoch 6/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4961 - acc: 0.8254Epoch 00006: val_loss improved from 0.36662 to 0.36344, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4959 - acc: 0.8255 - val_loss: 0.3634 - val_acc: 0.8605\n",
      "Epoch 7/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4917 - acc: 0.8260Epoch 00007: val_loss improved from 0.36344 to 0.36237, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4916 - acc: 0.8260 - val_loss: 0.3624 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4903 - acc: 0.8268Epoch 00008: val_loss improved from 0.36237 to 0.36155, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4904 - acc: 0.8267 - val_loss: 0.3615 - val_acc: 0.8597\n",
      "Epoch 9/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4811 - acc: 0.8289Epoch 00009: val_loss improved from 0.36155 to 0.36053, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4813 - acc: 0.8289 - val_loss: 0.3605 - val_acc: 0.8607\n",
      "Epoch 10/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.8271Epoch 00010: val_loss improved from 0.36053 to 0.35933, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4824 - acc: 0.8271 - val_loss: 0.3593 - val_acc: 0.8618\n",
      "Epoch 11/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4854 - acc: 0.8268Epoch 00011: val_loss improved from 0.35933 to 0.35862, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4852 - acc: 0.8268 - val_loss: 0.3586 - val_acc: 0.8603\n",
      "Epoch 12/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4829 - acc: 0.8283Epoch 00012: val_loss improved from 0.35862 to 0.35761, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4832 - acc: 0.8282 - val_loss: 0.3576 - val_acc: 0.8622\n",
      "Epoch 13/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4766 - acc: 0.8292Epoch 00013: val_loss improved from 0.35761 to 0.35672, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4768 - acc: 0.8292 - val_loss: 0.3567 - val_acc: 0.8618\n",
      "Epoch 14/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4771 - acc: 0.8292Epoch 00014: val_loss improved from 0.35672 to 0.35618, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4770 - acc: 0.8292 - val_loss: 0.3562 - val_acc: 0.8615\n",
      "Epoch 15/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4751 - acc: 0.8291Epoch 00015: val_loss improved from 0.35618 to 0.35572, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4751 - acc: 0.8292 - val_loss: 0.3557 - val_acc: 0.8620\n",
      "Epoch 16/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4758 - acc: 0.8290Epoch 00016: val_loss improved from 0.35572 to 0.35501, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4761 - acc: 0.8288 - val_loss: 0.3550 - val_acc: 0.8625\n",
      "Epoch 17/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4724 - acc: 0.8303Epoch 00017: val_loss improved from 0.35501 to 0.35379, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4726 - acc: 0.8303 - val_loss: 0.3538 - val_acc: 0.8630\n",
      "Epoch 18/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4754 - acc: 0.8304Epoch 00018: val_loss improved from 0.35379 to 0.35317, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4756 - acc: 0.8304 - val_loss: 0.3532 - val_acc: 0.8623\n",
      "Epoch 19/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4702 - acc: 0.8321Epoch 00019: val_loss improved from 0.35317 to 0.35267, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4705 - acc: 0.8320 - val_loss: 0.3527 - val_acc: 0.8630\n",
      "Epoch 20/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.8323Epoch 00020: val_loss improved from 0.35267 to 0.35244, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4705 - acc: 0.8322 - val_loss: 0.3524 - val_acc: 0.8627\n",
      "Epoch 21/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4720 - acc: 0.8311Epoch 00021: val_loss improved from 0.35244 to 0.35226, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4721 - acc: 0.8311 - val_loss: 0.3523 - val_acc: 0.8642\n",
      "Epoch 22/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4618 - acc: 0.8356Epoch 00022: val_loss improved from 0.35226 to 0.35112, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4619 - acc: 0.8355 - val_loss: 0.3511 - val_acc: 0.8635\n",
      "Epoch 23/100\n",
      "421/421 [============================>.] - ETA: 0s - loss: 0.4673 - acc: 0.8346Epoch 00023: val_loss improved from 0.35112 to 0.34996, saving model to MobileNet.h5\n",
      "422/421 [==============================] - 15s 36ms/step - loss: 0.4672 - acc: 0.8346 - val_loss: 0.3500 - val_acc: 0.8643\n",
      "Epoch 24/100\n",
      "287/421 [===================>..........] - ETA: 4s - loss: 0.4680 - acc: 0.8347"
     ]
    }
   ],
   "source": [
    "def run(model_name, lr, optimizer, epoch, patience, batch_size):\n",
    "    # Loading Datasets\n",
    "    (x_train, y_train), (x_val, y_val), (x_test, y_test) = load_data()\n",
    "\n",
    "    # Compute the bottleneck feature\n",
    "    input_shape = x_train.shape[1:]\n",
    "    n_class = 10\n",
    "\n",
    "    def get_features(MODEL, data=x_train):\n",
    "        cnn_model = MODEL(\n",
    "            include_top=False,\n",
    "            input_shape=input_shape,\n",
    "            weights=None)\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "        x = inputs\n",
    "        x = Lambda(preprocess_input, name='preprocessing')(x)\n",
    "        x = cnn_model(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        cnn_model = Model(inputs, x)\n",
    "\n",
    "        features = cnn_model.predict(data, batch_size=32, verbose=1)\n",
    "        return features\n",
    "\n",
    "    def fine_tune(MODEL,\n",
    "                  model_name,\n",
    "                  optimizer,\n",
    "                  lr,\n",
    "                  epoch,\n",
    "                  patience,\n",
    "                  batch_size,\n",
    "                  X=x_train):\n",
    "        # Fine-tune the model\n",
    "        print(\"\\n\\n Fine tune \" + model_name + \" : \\n\")\n",
    "\n",
    "        datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocess_input,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2)\n",
    "\n",
    "        val_datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocess_input)\n",
    "\n",
    "        inputs = Input(input_shape)\n",
    "        x = inputs\n",
    "        cnn_model = MODEL(\n",
    "            include_top=False,\n",
    "            input_shape=input_shape,\n",
    "            weights=None)\n",
    "        x = cnn_model(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(n_class, activation='softmax', name='predictions')(x)\n",
    "        model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "        # for layer in model.layers[:114]:\n",
    "        #     layer.trainable = False\n",
    "\n",
    "        try:\n",
    "            model.load_weights(model_name + '.h5')\n",
    "            print('Load ' + model_name + '.h5 successfully.')\n",
    "        except:\n",
    "            try:\n",
    "                model.load_weights('fc_' + model_name + '.h5', by_name=True)\n",
    "                print('Fail to load ' + model_name + '.h5, load fc_' +\n",
    "                      model_name + '.h5 instead.')\n",
    "            except:\n",
    "                print(\n",
    "                    'Start computing ' + model_name + ' bottleneck feature: ')\n",
    "                features = get_features(MODEL, X)\n",
    "\n",
    "                # Training models\n",
    "                inputs = Input(features.shape[1:])\n",
    "                x = inputs\n",
    "                x = Dropout(0.5)(x)\n",
    "                x = Dense(n_class, activation='softmax', name='predictions')(x)\n",
    "                model_fc = Model(inputs, x)\n",
    "                model_fc.compile(\n",
    "                    optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "                h = model_fc.fit(\n",
    "                    features,\n",
    "                    y,\n",
    "                    batch_size=128,\n",
    "                    epochs=5,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "                model_fc.save('fc_' + model_name + '.h5', 'w')\n",
    "\n",
    "        print(\"\\n \" + \"Optimizer=\" + optimizer + \" lr=\" + str(lr) + \" \\n\")\n",
    "        optimizer\n",
    "        if optimizer == \"Nadam\":\n",
    "            model.compile(\n",
    "                optimizer=Nadam(lr=lr),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "        elif optimizer == \"SGD\":\n",
    "            model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer=SGD(lr=lr, momentum=0.9, nesterov=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "        class LossHistory(keras.callbacks.Callback):\n",
    "            def on_train_begin(self, logs={}):\n",
    "                # self.val_losses = []\n",
    "                self.losses = []\n",
    "\n",
    "            def on_epoch_end(self, batch, logs={}):\n",
    "                # self.val_losses.append(logs.get(\"val_loss\"))\n",
    "                self.losses.append((logs.get('loss'), logs.get(\"val_loss\")))\n",
    "\n",
    "        history = LossHistory()\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss', patience=patience, verbose=1, mode='auto')\n",
    "        checkpointer = ModelCheckpoint(\n",
    "            filepath=model_name + '.h5', verbose=1, save_best_only=True)\n",
    "        h2 = model.fit_generator(\n",
    "            datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "            steps_per_epoch=len(x_train) / batch_size,\n",
    "            validation_data=val_datagen.flow(\n",
    "                x_val, y_val, batch_size=batch_size),\n",
    "            validation_steps=len(x_val) / batch_size,\n",
    "            epochs=epoch,\n",
    "            callbacks=[early_stopping, checkpointer, history])\n",
    "        #         print(history.losses)\n",
    "        # np.savetxt(\"val_losses_\" + str(optimizer) + \"_\" + str(lr) + \".csv\",\n",
    "        #            history.losses)\n",
    "        with open(model_name + \".csv\", 'a') as f_handle:\n",
    "            np.savetxt(f_handle, history.losses)\n",
    "        # with open(model_name + \"2.csv\", 'a') as f_handle:\n",
    "        #     np.savetxt(f_handle, model_name + str(optimizer) + \"_\" + str(lr))\n",
    "        #     np.savetxt(f_handle, history.losses)\n",
    "        # model.save(model_name + '.h5', 'w')\n",
    "\n",
    "    # list_model = [Xception, InceptionV3, InceptionResNetV2]\n",
    "    # list_name_model = [\"Xception\", \"InceptionV3\", \"InceptionResNetV2\"]\n",
    "    #\n",
    "    # for x in range(3):\n",
    "    list_model = {\n",
    "        \"Xception\": Xception,\n",
    "        \"InceptionV3\": InceptionV3,\n",
    "        \"InceptionResNetV2\": InceptionResNetV2,\n",
    "        \"VGG16\": VGG16,\n",
    "        \"MobileNet\": MobileNet\n",
    "    }\n",
    "    print(list_model[model_name])\n",
    "    fine_tune(list_model[model_name], model_name, optimizer, lr, epoch,\n",
    "              patience, batch_size, x_train)\n",
    "    # fine_tune(list_model[0], list_name_model[0], optimizer, lr, epoch,\n",
    "    #           patience, batch_size, X)\n",
    "\n",
    "\n",
    "\n",
    "run(\"MobileNet\", 5e-04, \"SGD\", 10000, 5, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training preprocess visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(h.history['loss'])\n",
    "plt.plot(h.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(h.history['acc'])\n",
    "plt.plot(h.history['val_acc'])\n",
    "plt.legend(['acc', 'val_acc'])\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
